from __future__ import print_function
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import time
from grakel import GraphKernel
import faiss
import random
from ogb.nodeproppred import PygNodePropPredDataset

# ğŸ”½ æ³¨å†Œå…¨å±€å…è®¸ååºåˆ—åŒ–çš„ PyG ç±»å‹
from torch_geometric.data.data import Data, DataEdgeAttr
from torch.serialization import add_safe_globals
add_safe_globals([Data, DataEdgeAttr])  # å¿…é¡»åœ¨ PygNodePropPredDataset åˆå§‹åŒ–å‰æ‰§è¡Œ

from torch_geometric.datasets import GEDDataset
import torch
from sklearn.decomposition import KernelPCA
from sklearn.impute import SimpleImputer

# ä¿å­˜åŸå§‹ torch.load å‡½æ•°
_original_torch_load = torch.load

num_subgraphs = 5000
subgraph_size = 10

# è¦†ç›– torch.loadï¼Œé»˜è®¤å¼ºåˆ¶ weights_only=False
def patched_torch_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _original_torch_load(*args, **kwargs)

# åº”ç”¨ monkey patch
torch.load = patched_torch_load

def pyg_to_networkx(data):
    G = nx.Graph()
    edges = data.edge_index.numpy()
    for u, v in zip(edges[0], edges[1]):
        G.add_edge(u, v)
    return G

def load_ogbn_arxiv():
    dataset = PygNodePropPredDataset(name="ogbn-arxiv")
    split_idx = dataset.get_idx_split()

    data, slices = torch.load(dataset.processed_paths[0])
    
    # å‡è®¾æ•°æ®é›†ä¸­åªæœ‰ä¸€ä¸ªå›¾
    G = pyg_to_networkx(data)
    labels = data.y.numpy()  # è·å–æ ‡ç­¾
    
    return G, labels, split_idx

def sample_subgraphs(G, num_subgraphs=5000, subgraph_size=10):
    nodes = list(G.nodes())
    subgraphs = []
    for _ in range(num_subgraphs):
        center = random.choice(nodes)
        neighbors = nx.single_source_shortest_path_length(G, center, cutoff=2).keys()
        neighbors = list(neighbors)
        if len(neighbors) > subgraph_size:
            neighbors = random.sample(neighbors, subgraph_size)
        SG = G.subgraph(neighbors).copy()
        subgraphs.append(SG)
    return subgraphs

def nx_to_grakel(subgraphs):
    gk_graphs = []
    for sg in subgraphs:
        adj_dict = {i: list(sg.neighbors(i)) for i in sg.nodes()}
        labels = {i: str(i) for i in sg.nodes()}  # ä½¿ç”¨èŠ‚ç‚¹ ID çš„å­—ç¬¦ä¸²ä½œä¸ºæ ‡ç­¾
        gk_graphs.append((adj_dict, labels))  # æ·»åŠ æ ‡ç­¾å­—å…¸
    return gk_graphs

def kernel_to_embedding(K, dim=128):
    # ä½¿ç”¨ SimpleImputer å¡«å…… NaN å€¼
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    K_imputed = imputer.fit_transform(K)
    
    # ä½¿ç”¨ KernelPCA è¿›è¡Œé™ç»´
    kpca = KernelPCA(n_components=dim, kernel='precomputed')
    embedding = kpca.fit_transform(K_imputed)
    return embedding

def random_walk_embedding(grakel_graphs):
    rw_kernel = GraphKernel(kernel={"name": "random_walk", "with_labels": False}, normalize=True)
    K = rw_kernel.fit_transform(grakel_graphs)
    embeddings = kernel_to_embedding(K, dim=128)  # ä½¿ç”¨ KernelPCA é™ç»´
    return embeddings

def wl_embedding(grakel_graphs):
    wl_kernel = GraphKernel(kernel={"name": "weisfeiler_lehman", "n_iter": 5}, normalize=True)
    K = wl_kernel.fit_transform(grakel_graphs)
    embeddings = kernel_to_embedding(K, dim=128)  # ä½¿ç”¨ KernelPCA é™ç»´
    return embeddings

# def random_walk_embedding(grakel_graphs):
#     rw_kernel = GraphKernel(kernel={"name": "random_walk", "with_labels": False}, normalize=True)
#     K = rw_kernel.fit_transform(grakel_graphs)
#     return K

# def wl_embedding(grakel_graphs):
#     wl_kernel = GraphKernel(kernel={"name": "weisfeiler_lehman", "n_iter": 5}, normalize=True)
#     K = wl_kernel.fit_transform(grakel_graphs)
#     return K

def build_groundtruth(embeddings, query_vectors, k):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings.astype(np.float32))
    _, gt_indices = index.search(query_vectors.astype(np.float32), k)
    return gt_indices

def build_index(index_type, embeddings):
    dim = embeddings.shape[1]
    print("Building index of type:", index_type, "with dimension:", dim)
    if index_type == 'Flat':
        index = faiss.IndexFlatL2(dim)
        index.add(embeddings.astype(np.float32))
    elif index_type == 'HNSW':
        index = faiss.IndexHNSWFlat(dim, 32)
        index.hnsw.efConstruction = 200
        index.hnsw.efSearch = 64
        index.add(embeddings.astype(np.float32))
    elif index_type == 'NSG':
        index = faiss.IndexNSGFlat(dim, 32)
        index.nsg.search_L = 64
        index.add(embeddings.astype(np.float32))
    elif index_type == 'IVFPQ':
        quantizer = faiss.IndexFlatL2(dim)
        index = faiss.IndexIVFPQ(quantizer, dim, 100, 8, 8)
        index.train(embeddings.astype(np.float32))
        index.add(embeddings.astype(np.float32))
        index.nprobe = 10
    else:
        raise ValueError("Unknown index type:", index_type)
    return index

def compute_recall(pred_indices, gt_indices):
    recalls = []
    for pred, gt in zip(pred_indices, gt_indices):
        correct = len(set(pred) & set(gt))
        recalls.append(correct / float(len(gt)))
    return np.mean(recalls)

def test(index_type, kernel_type, embeddings, query_embeddings, gt_indices, k):
    index = build_index(index_type, embeddings)
    print(f"Index built for {index_type} with kernel {kernel_type}.")
    start = time.time()
    _, pred_indices = index.search(query_embeddings.astype(np.float32), k)
    end = time.time()
    search_time = (end - start) * 1000
    recall = compute_recall(pred_indices, gt_indices)
    return recall, search_time

if __name__ == "__main__":
    # åŠ è½½ ogbn-arxiv æ•°æ®é›†
    G, labels, split_idx = load_ogbn_arxiv()
    print("Graph loaded. Nodes:", G.number_of_nodes(), "Edges:", G.number_of_edges())

    # é‡‡æ ·å­å›¾

    subgraphs = sample_subgraphs(G, num_subgraphs, subgraph_size)
    print("Sampled", len(subgraphs), "subgraphs.")

    # å°†å­å›¾è½¬æ¢ä¸º Grakel æ ¼å¼
    grakel_graphs = nx_to_grakel(subgraphs)

    kernels = {
        "RandomWalk": random_walk_embedding,
        "WL": wl_embedding
    }

    indices = ["Flat", "NSG", "HNSW", "IVFPQ"]
    k = 10
    results = {}

    for kernel_name, embed_func in kernels.items():
        print(f"Processing Kernel: {kernel_name}")
        embeddings = embed_func(grakel_graphs)
        query_embeddings = embeddings[:100]
        gt_indices = build_groundtruth(embeddings, query_embeddings, k)

        for index_type in indices:
            print(f"Testing Index: {index_type} with Kernel: {kernel_name}")
            recall, search_time = test(index_type, kernel_name, embeddings, query_embeddings, gt_indices, k)
            results[(kernel_name, index_type)] = (recall, search_time)

    # ç”»å›¾
    plt.figure(figsize=(10, 6))

    markers = {'Flat': 'o', 'NSG': 's', 'HNSW': '^', 'IVFPQ': 'x'}
    colors = {'RandomWalk': 'blue', 'WL': 'green'}

    for (kernel_name, index_type), (recall, search_time) in results.items():
        plt.scatter(search_time, recall, marker=markers[index_type], color=colors[kernel_name],
                    label=f"{kernel_name}-{index_type}")
        print(f"{kernel_name}-{index_type}: Recall@{k} = {recall:.4f}, Time = {search_time:.2f} ms")

    plt.xlabel('Search Time (ms)')
    plt.ylabel(f'Recall@{k}')
    plt.title('Recall vs Search Time (Different Kernels and Indexes)')
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    # æ„é€ åŒ…å«å‚æ•°çš„æ–‡ä»¶å
    filename = f"recall_vs_time_k{k}_num_subgraphs{num_subgraphs}_subgraph_size{subgraph_size}.png"
    plt.savefig(filename)
    print(f"Plot saved as {filename}")
    plt.show()